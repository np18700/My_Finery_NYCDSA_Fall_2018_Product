{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier\n",
    "\n",
    "### Training model on Whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data into dataframe\n",
    "\n",
    "data = pd.read_csv('new_total_train.csv') #training data\n",
    "finery = pd.read_csv('1210test_data.csv') #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brandnamestore</th>\n",
       "      <td>absbyallenschwartz ellie estrella womens shoes</td>\n",
       "      <td>absbyallenschwartz ellie samantha womens shoes</td>\n",
       "      <td>absbyallenschwartz ellie amada-l womens shoes</td>\n",
       "      <td>absbyallenschwartz ellie payton womens shoes</td>\n",
       "      <td>absbyallenschwartz ellie clarity womens shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categoryId</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itemName</th>\n",
       "      <td>ellie estrella womens</td>\n",
       "      <td>ellie samantha womens</td>\n",
       "      <td>ellie amada l womens</td>\n",
       "      <td>ellie payton womens</td>\n",
       "      <td>ellie clarity womens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brandnamestore_tokens</th>\n",
       "      <td>['ellie', 'estrella', 'womens', 'shoes']</td>\n",
       "      <td>['ellie', 'samantha', 'womens', 'shoes']</td>\n",
       "      <td>['ellie', 'amada', 'womens', 'shoes']</td>\n",
       "      <td>['ellie', 'payton', 'womens', 'shoes']</td>\n",
       "      <td>['ellie', 'clarity', 'womens', 'shoes']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brandnamestore_tokens_clean</th>\n",
       "      <td>ellie estrella womens shoes</td>\n",
       "      <td>ellie samantha womens shoes</td>\n",
       "      <td>ellie amada womens shoes</td>\n",
       "      <td>ellie payton womens shoes</td>\n",
       "      <td>ellie clarity womens shoes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          0  \\\n",
       "brandnamestore               absbyallenschwartz ellie estrella womens shoes   \n",
       "categoryId                                                              200   \n",
       "itemName                                              ellie estrella womens   \n",
       "brandnamestore_tokens              ['ellie', 'estrella', 'womens', 'shoes']   \n",
       "brandnamestore_tokens_clean                     ellie estrella womens shoes   \n",
       "\n",
       "                                                                          1  \\\n",
       "brandnamestore               absbyallenschwartz ellie samantha womens shoes   \n",
       "categoryId                                                              200   \n",
       "itemName                                              ellie samantha womens   \n",
       "brandnamestore_tokens              ['ellie', 'samantha', 'womens', 'shoes']   \n",
       "brandnamestore_tokens_clean                     ellie samantha womens shoes   \n",
       "\n",
       "                                                                         2  \\\n",
       "brandnamestore               absbyallenschwartz ellie amada-l womens shoes   \n",
       "categoryId                                                             200   \n",
       "itemName                                              ellie amada l womens   \n",
       "brandnamestore_tokens                ['ellie', 'amada', 'womens', 'shoes']   \n",
       "brandnamestore_tokens_clean                       ellie amada womens shoes   \n",
       "\n",
       "                                                                        3  \\\n",
       "brandnamestore               absbyallenschwartz ellie payton womens shoes   \n",
       "categoryId                                                            200   \n",
       "itemName                                              ellie payton womens   \n",
       "brandnamestore_tokens              ['ellie', 'payton', 'womens', 'shoes']   \n",
       "brandnamestore_tokens_clean                     ellie payton womens shoes   \n",
       "\n",
       "                                                                         4  \n",
       "brandnamestore               absbyallenschwartz ellie clarity womens shoes  \n",
       "categoryId                                                             200  \n",
       "itemName                                              ellie clarity womens  \n",
       "brandnamestore_tokens              ['ellie', 'clarity', 'womens', 'shoes']  \n",
       "brandnamestore_tokens_clean                     ellie clarity womens shoes  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at data\n",
    "\n",
    "data.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brandnamestore                 0\n",
       "categoryId                     0\n",
       "itemName                       0\n",
       "brandnamestore_tokens          0\n",
       "brandnamestore_tokens_clean    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking Nan values\n",
    "data.isnull().sum()\n",
    "\n",
    "#found one missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with Nan values\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is Nan in one row in finery test data after tokenization, so i filled that with empty string.\n",
    "\n",
    "finery.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning rain data \n",
    "\n",
    "X = data['brandnamestore_tokens_clean']\n",
    "Y = data['categoryId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split for test and train data\n",
    "\n",
    "# try:\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "# except: \n",
    "#     from sklearn.cross_validation import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59601        newyork black dot print twist jumpsuit newyork\n",
       "101710                        monki cat cross body bag asos\n",
       "126481                  tailorvintage anchor away tee offth\n",
       "106533    darby home brookstonval drawer kitchen island ...\n",
       "15537             nicolemiller barletta bootie nicolemiller\n",
       "Name: brandnamestore_tokens_clean, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7930                     frye ribbed hat denim orchardmile\n",
       "15944               nike essentials womens parka nordstrom\n",
       "99984    aquazzura rendezvous flat sandals saksfifthavenue\n",
       "64352                    rickowens fishtail parka farfetch\n",
       "56783    driesvannoten embossed leather handle bag ther...\n",
       "Name: brandnamestore_tokens_clean, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43201,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100801,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking product token for test data \n",
    "\n",
    "finery_test = finery.brandnamestore_tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing word 'Top' from stopwords and create myown stop words\n",
    "# because 'Top' is a women's clothing keyword\n",
    "\n",
    "mystopwords = ['all',\n",
    " 'amount',\n",
    " 'am',\n",
    " 'might',\n",
    " 'give',\n",
    " 'here',\n",
    " 'alone',\n",
    " 'nobody',\n",
    " 'formerly',\n",
    " 'indeed',\n",
    " 'made',\n",
    " 'seeming',\n",
    " 'five',\n",
    " 'with',\n",
    " 'amongst',\n",
    " 'somewhere',\n",
    " 'someone',\n",
    " 'an',\n",
    " 'he',\n",
    " 'hereafter',\n",
    " 'empty',\n",
    " 'their',\n",
    " 'among',\n",
    " 'out',\n",
    " 'themselves',\n",
    " 'bottom',\n",
    " 'no',\n",
    " 'make',\n",
    " 'behind',\n",
    " 'co',\n",
    " 'to',\n",
    " 'they',\n",
    " 'anyhow',\n",
    " 'each',\n",
    " 'detail',\n",
    " 'sixty',\n",
    " 'her',\n",
    " 'what',\n",
    " 'would',\n",
    " 'sincere',\n",
    " 'this',\n",
    " 'for',\n",
    " 'across',\n",
    " 'least',\n",
    " 'too',\n",
    " 'any',\n",
    " 'up',\n",
    " 'becomes',\n",
    " 'myself',\n",
    " 'those',\n",
    " 'some',\n",
    " 'upon',\n",
    " 'onto',\n",
    " 'therein',\n",
    " 'herein',\n",
    " 'many',\n",
    " 'third',\n",
    " 'along',\n",
    " 'will',\n",
    " 're',\n",
    " 'ours',\n",
    " 'already',\n",
    " 'before',\n",
    " 'during',\n",
    " 'twenty',\n",
    " 'whence',\n",
    " 'though',\n",
    " 'last',\n",
    " 'none',\n",
    " 'serious',\n",
    " 'show',\n",
    " 'less',\n",
    " 'a',\n",
    " 'i',\n",
    " 'enough',\n",
    " 'since',\n",
    " 'first',\n",
    " 'must',\n",
    " 'whenever',\n",
    " 'cannot',\n",
    " 'nor',\n",
    " 'should',\n",
    " 'thereupon',\n",
    " 'it',\n",
    " 'put',\n",
    " 'whatever',\n",
    " 'until',\n",
    " 'something',\n",
    " 'unless',\n",
    " 'much',\n",
    " 'therefore',\n",
    " 'fire',\n",
    " 'every',\n",
    " 'still',\n",
    " 'hers',\n",
    " 'latterly',\n",
    " 'whole',\n",
    " 'so',\n",
    " 'bill',\n",
    " 'go',\n",
    " 'own',\n",
    " 'eight',\n",
    " 'is',\n",
    " 'us',\n",
    " 'beside',\n",
    " 'whereafter',\n",
    " 'yet',\n",
    " 'anyway',\n",
    " 'she',\n",
    " 'perhaps',\n",
    " 'why',\n",
    " 'both',\n",
    " 'hundred',\n",
    " 'four',\n",
    " 'namely',\n",
    " 'who',\n",
    " 'its',\n",
    " 'over',\n",
    " 'there',\n",
    " 'my',\n",
    " 'whereas',\n",
    " 'doing',\n",
    " 'say',\n",
    " 'fify',\n",
    " 'except',\n",
    " 'most',\n",
    " 'besides',\n",
    " 'another',\n",
    " 'describe',\n",
    " 'only',\n",
    " 'these',\n",
    " 'nowhere',\n",
    " 'may',\n",
    " 'everyone',\n",
    " 'ltd',\n",
    " 'been',\n",
    " 'hasnt',\n",
    " 'was',\n",
    " 'towards',\n",
    " 'never',\n",
    " 'were',\n",
    " 'under',\n",
    " 'we',\n",
    " 'if',\n",
    " 'several',\n",
    " 'that',\n",
    " 'computer',\n",
    " 'side',\n",
    " 'elsewhere',\n",
    " 'un',\n",
    " 'wherein',\n",
    " 'see',\n",
    " 'well',\n",
    " 'between',\n",
    " 'amoungst',\n",
    " 'else',\n",
    " 'his',\n",
    " 'always',\n",
    " 'throughout',\n",
    " 'be',\n",
    " 'ever',\n",
    " 'wherever',\n",
    " 'itself',\n",
    " 'did',\n",
    " 'via',\n",
    " 'had',\n",
    " 'whom',\n",
    " 'being',\n",
    " 'back',\n",
    " 'get',\n",
    " 'your',\n",
    " 'and',\n",
    " 'former',\n",
    " 'six',\n",
    " 'etc',\n",
    " 'everywhere',\n",
    " 'now',\n",
    " 'while',\n",
    " 'same',\n",
    " 'doesn',\n",
    " 'whither',\n",
    " 'beforehand',\n",
    " 'using',\n",
    " 'anything',\n",
    " 'didn',\n",
    " 'sometime',\n",
    " 'hence',\n",
    " 'done',\n",
    " 'inc',\n",
    " 'other',\n",
    " 'really',\n",
    " 'part',\n",
    " 'ie',\n",
    " 'hereupon',\n",
    " 'thru',\n",
    " 'about',\n",
    " 'where',\n",
    " 'found',\n",
    " 'them',\n",
    " 'mill',\n",
    " 'name',\n",
    " 'become',\n",
    " 'thence',\n",
    " 'ten',\n",
    " 'further',\n",
    " 'system',\n",
    " 'next',\n",
    " 'call',\n",
    " 'below',\n",
    " 'on',\n",
    " 'because',\n",
    " 'as',\n",
    " 'again',\n",
    " 'above',\n",
    " 'otherwise',\n",
    " 'seems',\n",
    " 'three',\n",
    " 'mostly',\n",
    " 'the',\n",
    " 'take',\n",
    " 'our',\n",
    " 'although',\n",
    " 'once',\n",
    " 'one',\n",
    " 'per',\n",
    " 'also',\n",
    " 'few',\n",
    " 'are',\n",
    " 'yours',\n",
    " 'has',\n",
    " 'ourselves',\n",
    " 'km',\n",
    " 'how',\n",
    " 'yourself',\n",
    " 'than',\n",
    " 'full',\n",
    " 'meanwhile',\n",
    " 'thus',\n",
    " 'however',\n",
    " 'within',\n",
    " 'two',\n",
    " 'regarding',\n",
    " 'various',\n",
    " 'moreover',\n",
    " 'himself',\n",
    " 'at',\n",
    " 'anywhere',\n",
    " 'con',\n",
    " 'noone',\n",
    " 'without',\n",
    " 'cant',\n",
    " 'do',\n",
    " 'mine',\n",
    " 'often',\n",
    " 'herself',\n",
    " 'rather',\n",
    " 'whether',\n",
    " 'toward',\n",
    " 'twelve',\n",
    " 'around',\n",
    " 'nine',\n",
    " 'thereafter',\n",
    " 'against',\n",
    " 'into',\n",
    " 'eg',\n",
    " 'others',\n",
    " 'of',\n",
    " 'after',\n",
    " 'de',\n",
    " 'front',\n",
    " 'seem',\n",
    " 'in',\n",
    " 'whereby',\n",
    " 'or',\n",
    " 'used',\n",
    " 'could',\n",
    " 'whoever',\n",
    " 'eleven',\n",
    " 'such',\n",
    " 'whose',\n",
    " 'thin',\n",
    " 'but',\n",
    " 'off',\n",
    " 'latter',\n",
    " 'even',\n",
    " 'cry',\n",
    " 'anyone',\n",
    " 'afterwards',\n",
    " 'becoming',\n",
    " 'yourselves',\n",
    " 'quite',\n",
    " 'thick',\n",
    " 'forty',\n",
    " 'couldnt',\n",
    " 'find',\n",
    " 'neither',\n",
    " 'became',\n",
    " 'hereby',\n",
    " 'not',\n",
    " 'which',\n",
    " 'then',\n",
    " 'him',\n",
    " 'more',\n",
    " 'somehow',\n",
    " 'everything',\n",
    " 'by',\n",
    " 'together',\n",
    " 'interest',\n",
    " 'fifteen',\n",
    " 'keep',\n",
    " 'kg',\n",
    " 'nevertheless',\n",
    " 'can',\n",
    " 'you',\n",
    " 'have',\n",
    " 'sometimes',\n",
    " 'either',\n",
    " 'does',\n",
    " 'when',\n",
    " 'thereby',\n",
    " 'don',\n",
    " 'seemed',\n",
    " 'move',\n",
    " 'from',\n",
    " 'almost',\n",
    " 'nothing',\n",
    " 'down',\n",
    " 'very',\n",
    " 'please',\n",
    " 'me',\n",
    " 'just',\n",
    " 'fill',\n",
    " 'due',\n",
    " 'through',\n",
    " 'whereupon',\n",
    " 'beyond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running SGD Classifier on best parameters of grid search\n",
    "#Making Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "p = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer(stop_words=mystopwords,use_idf=True, min_df = 5,max_df = 0.5, ngram_range = (1,3))),\n",
    "    ('sgd', SGDClassifier(alpha=0.00001, average=False, class_weight='balanced', epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
    "       learning_rate='optimal', loss='log', max_iter=1000, n_iter=None,\n",
    "       n_jobs=-1, penalty='l2', power_t=0.5, random_state=None,\n",
    "       shuffle=True, tol=0.001, verbose=0, warm_start=False))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...'l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=0.001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting model on entire Data\n",
    "\n",
    "p.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 40580 / 43201 correct [93.93%]\n"
     ]
    }
   ],
   "source": [
    "#used for train test split\n",
    "\n",
    "# predictions = p.predict(X_test)\n",
    "\n",
    "# correct = sum(predictions == y_test)\n",
    "# incorrect = len(predictions) - correct\n",
    "# print(('got {} / {} correct [{:.2%}]').format(correct, correct + incorrect, correct / float(correct + incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393301080993496"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(200.0, 0.9599866065313555)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking probability of word or product name\n",
    "\n",
    "predictions = p.predict_proba(['boots'])\n",
    "list(filter(lambda x: x[1] > 0.1, zip(p.classes_, predictions[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00904292e-02, 2.49628949e-03, 1.36785002e-03, 2.74999784e-03,\n",
       "        1.57923963e-03, 1.54734583e-03, 2.96971640e-03, 3.31263252e-03,\n",
       "        9.59986607e-01, 1.50115511e-03, 1.64021623e-03, 2.88073897e-03,\n",
       "        1.61885280e-03, 1.59842126e-04, 6.09908729e-03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of product to belong in each category\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting finest test data\n",
    "\n",
    "pred = p.predict(finery_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new 'Predict' column in finery dataframe\n",
    "\n",
    "finery['Predict'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing output into 'csv' file\n",
    "\n",
    "finery.to_csv('SGD1212w_predictnp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
